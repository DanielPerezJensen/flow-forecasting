{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8e5d387",
   "metadata": {},
   "source": [
    "## In this notebook we implement some simple baselines\n",
    "\n",
    "Including ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import dates as mdates\n",
    "import collections\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afc905d",
   "metadata": {},
   "source": [
    "### Load in data \n",
    "\n",
    "First we load the data from NDSI, NDVI and DGA and only consider years after 1965, as there is insufficient data before 1965."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0195e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_folder_path = os.path.join(\"..\", \"data\", \"processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898c7205",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NDSI = pd.read_csv(os.path.join(processed_folder_path, \"NDSI.csv\"), index_col=0, parse_dates=[\"date\"])\n",
    "df_DGA = pd.read_csv(os.path.join(processed_folder_path, \"DGA.csv\"), index_col=0, parse_dates=[\"date\"])\n",
    "df_NDVI = pd.read_csv(os.path.join(processed_folder_path, \"NDSI.csv\"), index_col=0, parse_dates=[\"date\"])\n",
    "\n",
    "df_NDSI = df_NDSI.loc[df_NDSI[\"date\"].dt.year >= 1965]\n",
    "df_NDVI = df_NDVI.loc[df_NDVI[\"date\"].dt.year >= 1965]\n",
    "df_DGA = df_DGA.loc[df_DGA[\"date\"].dt.year >= 1965]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271e0cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_flow_data_mean = df_DGA.groupby(pd.PeriodIndex(df_DGA['date'], freq=\"M\"))['river_flow'].mean()\n",
    "monthly_flow_data_median = df_DGA.groupby(pd.PeriodIndex(df_DGA['date'], freq=\"M\"))['river_flow'].median()\n",
    "\n",
    "flow_mean_df = monthly_flow_data_mean.reset_index()\n",
    "flow_mean_df.date = pd.to_datetime(flow_mean_df.date.astype(\"str\"))\n",
    "\n",
    "# Offset date by 3 monvary widely by jurisdiction. In many countries such as the United Kingdom, the word is not generally used and, with the exception of certain high-speed roads, there are no laws lths, so 1 april aligns with the first day of a given water year\n",
    "flow_mean_df.date = flow_mean_df.date + pd.tseries.offsets.DateOffset(months=-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab39105",
   "metadata": {},
   "source": [
    "### Simple baseline using average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fead6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_mean_df.head(5)\n",
    "\n",
    "summer_avg = flow_mean_df.loc[flow_mean_df.date.dt.month > 6][\"river_flow\"].mean()\n",
    "print(f\"Average of all summer months: {summer_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3c0c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "\n",
    "for year in flow_mean_df.date.dt.year.unique():\n",
    "    year_rows = flow_mean_df.loc[flow_mean_df.date.dt.year == year][\"river_flow\"]\n",
    "    \n",
    "    if len(year_rows) == 12:\n",
    "        y.append(year_rows[6:].mean())\n",
    "\n",
    "preds = [summer_avg for _ in y]\n",
    "\n",
    "preds = torch.Tensor(preds)\n",
    "y = torch.Tensor(y)\n",
    "\n",
    "mse_loss = nn.functional.mse_loss(preds, y).item()\n",
    "\n",
    "print(f\"If we use the average of the summer months as a predictive model, we get an MSE loss of: \\n{mse_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643eac77",
   "metadata": {},
   "source": [
    "### MLP\n",
    "#### We will use Pytorch to implement some simple neural networks\n",
    "\n",
    "https://pytorch.org/\n",
    "\n",
    "First we refine the data and put it into a DataLoader object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d93ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed071a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data into usable format for Pytorch\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# 1987 seems to have insufficient data, so ignore years that do not contain 12 values\n",
    "for year in flow_mean_df.date.dt.year.unique():\n",
    "    year_rows = flow_mean_df.loc[flow_mean_df.date.dt.year == year][\"river_flow\"]\n",
    "    \n",
    "    if len(year_rows) == 12:\n",
    "        X.append(year_rows[:6])\n",
    "        y.append(year_rows[6:].mean())\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96539c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiverFlowDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = np.float32(X)\n",
    "        self.y = np.float32(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd4a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(inputs, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, outputs)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348a6a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RiverFlowDataset(X, y)\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [round(len(dataset) * 0.8), round(len(dataset) * 0.2)])                                              \n",
    "\n",
    "dataloader = DataLoader(train_set, batch_size=2, shuffle=True, num_workers=2)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "mlp_model = MLP(6, 1).to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optim = torch.optim.Adam(mlp_model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in tqdm(range(0, 200), desc=\"Epoch\"):\n",
    "    \n",
    "    for i, data in enumerate(dataloader):\n",
    "        mlp_model.train()\n",
    "        inputs, targets = data\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        outputs = mlp_model(inputs)\n",
    "        \n",
    "        loss = loss_fn(outputs, targets.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86828881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, validation_set):\n",
    "    testloader = DataLoader(validation_set, batch_size=2, num_workers=2)\n",
    "    metric = nn.MSELoss()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for data in tqdm(testloader):\n",
    "        inputs, targets = data\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "    \n",
    "        total_loss += metric(outputs, targets.unsqueeze(1))\n",
    "        \n",
    "    test_loss = total_loss.item() / len(validation_set)\n",
    "    print(f\"Test Loss: {test_loss:.3f}\")\n",
    "    \n",
    "    \n",
    "# print(len(val_set))\n",
    "test(mlp_model, val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14686ce5",
   "metadata": {},
   "source": [
    "### RNN\n",
    "\n",
    "To properly use an RNN we will need to do some feature engineering. Using for example the datetime as features. Furthermore we will add NDSI NDVI at some point.\n",
    "\n",
    "First we generate out dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498f4578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lags(df, value, n_lags):\n",
    "    \"\"\"\n",
    "    generate_lags\n",
    "    Generates a dataframe with columns denoting lagged value up to n_lags\n",
    "    Args:\n",
    "        df: dataframe to lag\n",
    "        value: value to lag\n",
    "        n_lags: amount of rows to lag\n",
    "    \"\"\"\n",
    "    df_n = df.copy()\n",
    "    \n",
    "    for n in range(1, n_lags + 1):\n",
    "        df_n[f\"lag_{n}\"] = df_n[f\"{value}\"].shift(n)\n",
    "    \n",
    "    df_n = df_n.iloc[n_lags:]\n",
    "    \n",
    "    return df_n\n",
    "\n",
    "df_generated = generate_lags(flow_mean_df, \"river_flow\", 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dd8a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = (\n",
    "    df_generated\n",
    "    .assign(month = df_generated.date.dt.month)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44ad422",
   "metadata": {},
   "source": [
    "Given that the months in a year are cyclical we can represent the months using a sinus and cosine wave, this improves learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b82ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cyclical_features(df, col_name, period, start_num=0):\n",
    "    kwargs = {\n",
    "        f\"sin_{col_name}\" : lambda x: np.sin(2 * np.pi * (df[col_name] - start_num) / period),\n",
    "        f\"cos_{col_name}\" : lambda x: np.cos(2 * np.pi * (df[col_name] - start_num) / period)\n",
    "    }\n",
    "    \n",
    "    return df.assign(**kwargs).drop(columns=[col_name])\n",
    "\n",
    "df_features = generate_cyclical_features(df_features, \"month\", 12, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e859c182",
   "metadata": {},
   "source": [
    "Scaling the input data using various standard scalers improves learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler\n",
    "\n",
    "def get_scaler(scaler):\n",
    "    scalers = {\n",
    "        \"minmax\": MinMaxScaler,\n",
    "        \"standard\": StandardScaler,\n",
    "        \"maxabs\": MaxAbsScaler,\n",
    "        \"robust\": RobustScaler\n",
    "    }\n",
    "    \n",
    "    return scalers[scaler.lower()]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7059a4b",
   "metadata": {},
   "source": [
    "We create our own dataset which takes the features dataset and a scaler and is able to retrieve scaled datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5f7c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_dataset(Dataset):\n",
    "    def __init__(self, df, scaler=None):\n",
    "        x = df.iloc[:, 2:].values\n",
    "        y = df.iloc[:, 1].values[..., np.newaxis]\n",
    "        \n",
    "        if scaler:\n",
    "            x = scaler.fit_transform(x)\n",
    "            y = scaler.fit_transform(y)\n",
    "        \n",
    "            self.scaler = scaler\n",
    "        \n",
    "        self.x_train = torch.tensor(x, dtype=torch.float32)\n",
    "        self.y_train = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_train[idx], self.y_train[idx]\n",
    "\n",
    "scaler = get_scaler(\"minmax\")\n",
    "dataset = RNN_dataset(df_features)\n",
    "\n",
    "x, y = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e8d511",
   "metadata": {},
   "source": [
    "Below we define our GRU, which was graciously taken from the blogpost: https://towardsdatascience.com/building-rnn-lstm-and-gru-for-time-series-using-pytorch-a46e5b094e7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474ea35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super(GRUModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.layer_dim = layer_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # GRU layers\n",
    "        self.gru = nn.GRU(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        out, _ = self.gru(x, h0.detach())\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2713a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(dataset, [round(len(dataset) * 0.8), round(len(dataset) * 0.2)])                                              \n",
    "\n",
    "input_dim = len(train_set[0][0])\n",
    "hidden_dim = 64\n",
    "output_dim = 1\n",
    "layer_dim = 3\n",
    "batch_size = 1\n",
    "dropout_prob = 0.2\n",
    "n_epochs = 100\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-6\n",
    "\n",
    "dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "gru_model = GRUModel(input_dim, hidden_dim, layer_dim, output_dim, dropout_prob)\n",
    "loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "optim = torch.optim.Adam(gru_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "for epoch in tqdm(range(1, n_epochs + 1), desc=\"Epoch\"):\n",
    "    for i, data in enumerate(dataloader):\n",
    "        gru_model.train()\n",
    "        inputs, targets = data\n",
    "        inputs = inputs.view([batch_size, -1, input_dim])\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        outputs = gru_model(inputs)\n",
    "\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5defa98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gru(model, validation_set):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    testloader = DataLoader(validation_set, batch_size=batch_size, num_workers=2)\n",
    "    \n",
    "    for i, data in enumerate(tqdm(testloader)):\n",
    "        inputs, targets = data\n",
    "        inputs = inputs.view([batch_size, -1, input_dim])\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        loss = loss_fn(model(inputs), targets)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "                    \n",
    "    print(f\"Test Loss: {total_loss / len(validation_set):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4981e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gru(gru_model, val_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
