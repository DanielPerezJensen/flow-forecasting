{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8e5d387",
   "metadata": {},
   "source": [
    "## In this notebook we implement some simple baselines\n",
    "\n",
    "Including ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import dates as mdates\n",
    "import collections\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afc905d",
   "metadata": {},
   "source": [
    "### Load in data \n",
    "\n",
    "First we load the data from NDSI, NDVI and DGA and only consider years after 1965, as there is insufficient data before 1965."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0195e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_folder_path = os.path.join(\"..\", \"data\", \"processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898c7205",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NDSI = pd.read_csv(os.path.join(processed_folder_path, \"NDSI.csv\"), index_col=0, parse_dates=[\"date\"])\n",
    "df_DGA = pd.read_csv(os.path.join(processed_folder_path, \"DGA.csv\"), index_col=0, parse_dates=[\"date\"])\n",
    "df_NDVI = pd.read_csv(os.path.join(processed_folder_path, \"NDSI.csv\"), index_col=0, parse_dates=[\"date\"])\n",
    "\n",
    "df_NDSI = df_NDSI.loc[df_NDSI[\"date\"].dt.year >= 1965]\n",
    "df_NDVI = df_NDVI.loc[df_NDVI[\"date\"].dt.year >= 1965]\n",
    "df_DGA = df_DGA.loc[df_DGA[\"date\"].dt.year >= 1965]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271e0cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_flow_data_mean = df_DGA.groupby(pd.PeriodIndex(df_DGA['date'], freq=\"M\"))['river_flow'].mean()\n",
    "monthly_flow_data_median = df_DGA.groupby(pd.PeriodIndex(df_DGA['date'], freq=\"M\"))['river_flow'].median()\n",
    "\n",
    "flow_mean_df = monthly_flow_data_mean.reset_index()\n",
    "flow_mean_df.date = pd.to_datetime(flow_mean_df.date.astype(\"str\"))\n",
    "\n",
    "# Offset date by 3 monvary widely by jurisdiction. In many countries such as the United Kingdom, the word is not generally used and, with the exception of certain high-speed roads, there are no laws lths, so 1 april aligns with the first day of a given water year\n",
    "flow_mean_df.date = flow_mean_df.date + pd.tseries.offsets.DateOffset(months=-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a509a4aa",
   "metadata": {},
   "source": [
    "### Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ec20e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "643eac77",
   "metadata": {},
   "source": [
    "### MLP\n",
    "#### We will use Pytorch to implement some simple neural networks\n",
    "\n",
    "https://pytorch.org/\n",
    "\n",
    "First we refine the data and put it into a DataLoader object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d93ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b1f626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lags(df, value, n_lags):\n",
    "    \"\"\"\n",
    "    generate_lags\n",
    "    Generates a dataframe with columns denoting lagged value up to n_lags\n",
    "    Args:\n",
    "        df: dataframe to lag\n",
    "        value: value to lag\n",
    "        n_lags: amount of rows to lag\n",
    "    \"\"\"\n",
    "    df_n = df.copy()\n",
    "    \n",
    "    for n in range(1, n_lags + 1):\n",
    "        df_n[f\"lag_{n}\"] = df_n[f\"{value}\"].shift(n)\n",
    "    \n",
    "    df_n = df_n.iloc[n_lags:]\n",
    "    \n",
    "    return df_n\n",
    "\n",
    "df_generated_mlp = generate_lags(flow_mean_df, \"river_flow\", 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96539c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiverFlowDataset(Dataset):\n",
    "    def __init__(self, df, scaler=None):\n",
    "        x = df.iloc[:, 2:].values\n",
    "        y = df.iloc[:, 1].values[..., np.newaxis]\n",
    "        \n",
    "        if scaler:\n",
    "            x = scaler.fit_transform(x)\n",
    "            y = scaler.fit_transform(y)\n",
    "        \n",
    "            self.scaler = scaler\n",
    "        \n",
    "        self.x_train = torch.tensor(x, dtype=torch.float32)\n",
    "        self.y_train = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_train[idx], self.y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd4a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(inputs, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, outputs)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348a6a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mlp = RiverFlowDataset(df_generated_mlp)\n",
    "train_set_mlp, val_set_mlp = torch.utils.data.random_split(dataset_mlp, [round(len(dataset_mlp) * 0.8), round(len(dataset_mlp) * 0.2)])                                              \n",
    "\n",
    "dataloader_mlp = DataLoader(train_set_mlp, batch_size=2, shuffle=True, num_workers=2)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "mlp_model = MLP(12, 1).to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optim = torch.optim.Adam(mlp_model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in tqdm(range(0, 100), desc=\"Epoch\"):\n",
    "    \n",
    "    for i, data in enumerate(dataloader_mlp):\n",
    "        mlp_model.train()\n",
    "        inputs, targets = data\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        outputs = mlp_model(inputs)\n",
    "        \n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91a0ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def evaluate(model, validation_set, input_dim=None):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    predictions = []\n",
    "    values = []\n",
    "\n",
    "    testloader = DataLoader(validation_set, batch_size=1, num_workers=2)\n",
    "    \n",
    "    for i, data in enumerate(tqdm(testloader)):\n",
    "        inputs, targets = data\n",
    "        if input_dim:\n",
    "            inputs = inputs.view([1, -1, input_dim])\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        predictions.append(outputs.detach().numpy())\n",
    "        values.append(targets.detach().numpy())        \n",
    "                        \n",
    "    return predictions, values\n",
    "\n",
    "\n",
    "def format_predictions(predictions, values, scaler=None):\n",
    "    \"\"\"\n",
    "    Format predictions and values into dataframe for easy plotting\n",
    "    \"\"\"\n",
    "    vals = np.concatenate(values, axis=0).ravel()\n",
    "    preds = np.concatenate(predictions, axis=0).ravel()\n",
    "    \n",
    "    df_result = pd.DataFrame(data={\"value\": vals, \"prediction\": preds})\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    \"\"\"\n",
    "    Calculates various metrics on a df containing actual targets and predicted targets\n",
    "    \"\"\"\n",
    "    return {'mse': mean_squared_error(df.value, df.prediction),\n",
    "            'rmse' : mean_squared_error(df.value, df.prediction) ** 0.5,\n",
    "            'r2' : r2_score(df.value, df.prediction)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b78a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_mlp, values_mlp = evaluate(mlp_model, val_set_mlp)\n",
    "df_result_mlp = format_predictions(predictions_mlp, values_mlp)\n",
    "result_metrics_mlp = calculate_metrics(df_result_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d345adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Metrics of predicted values:\")\n",
    "for key, val in result_metrics_mlp.items():\n",
    "    print(f\"{key.upper()}: {val:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14686ce5",
   "metadata": {},
   "source": [
    "### RNN\n",
    "\n",
    "To properly use an RNN we will need to do some feature engineering. Using for example the datetime as features. Furthermore we will add NDSI NDVI at some point.\n",
    "\n",
    "First we generate out dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dd8a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generated_gru = generate_lags(flow_mean_df, \"river_flow\", 12)\n",
    "\n",
    "df_features_gru = (\n",
    "    df_generated_gru\n",
    "    .assign(month = df_generated_gru.date.dt.month)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44ad422",
   "metadata": {},
   "source": [
    "Given that the months in a year are cyclical we can represent the months using a sinus and cosine wave, this improves learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b82ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cyclical_features(df, col_name, period, start_num=0):\n",
    "    kwargs = {\n",
    "        f\"sin_{col_name}\" : lambda x: np.sin(2 * np.pi * (df[col_name] - start_num) / period),\n",
    "        f\"cos_{col_name}\" : lambda x: np.cos(2 * np.pi * (df[col_name] - start_num) / period)\n",
    "    }\n",
    "    \n",
    "    return df.assign(**kwargs).drop(columns=[col_name])\n",
    "\n",
    "df_features_gru = generate_cyclical_features(df_features_gru, \"month\", 12, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e859c182",
   "metadata": {},
   "source": [
    "Scaling the input data using various standard scalers improves learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler\n",
    "\n",
    "def get_scaler(scaler):\n",
    "    scalers = {\n",
    "        \"minmax\": MinMaxScaler,\n",
    "        \"standard\": StandardScaler,\n",
    "        \"maxabs\": MaxAbsScaler,\n",
    "        \"robust\": RobustScaler\n",
    "    }\n",
    "    \n",
    "    return scalers[scaler.lower()]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7059a4b",
   "metadata": {},
   "source": [
    "We create our own dataset which takes the features dataset and a scaler and is able to retrieve scaled datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5f7c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_gru = RiverFlowDataset(df_features_gru)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e8d511",
   "metadata": {},
   "source": [
    "Below we define our GRU, which was graciously taken from the blogpost: https://towardsdatascience.com/building-rnn-lstm-and-gru-for-time-series-using-pytorch-a46e5b094e7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474ea35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super(GRUModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.layer_dim = layer_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # GRU layers\n",
    "        self.gru = nn.GRU(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        out, _ = self.gru(x, h0.detach())\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abb968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_gru, val_set_gru = torch.utils.data.random_split(dataset_gru, [round(len(dataset_gru) * 0.8), round(len(dataset_gru) * 0.2)])                                              \n",
    "\n",
    "input_dim = len(train_set_gru[0][0])\n",
    "hidden_dim = 64\n",
    "output_dim = 1\n",
    "layer_dim = 3\n",
    "batch_size = 1\n",
    "dropout_prob = 0.2\n",
    "n_epochs = 100\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-6\n",
    "\n",
    "dataloader_gru = DataLoader(train_set_gru, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "gru_model = GRUModel(input_dim, hidden_dim, layer_dim, output_dim, dropout_prob)\n",
    "loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "optim = torch.optim.Adam(gru_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "for epoch in tqdm(range(1, n_epochs + 1), desc=\"Epoch\"):\n",
    "    for i, data in enumerate(dataloader_gru):\n",
    "        gru_model.train()\n",
    "        inputs, targets = data\n",
    "        inputs = inputs.view([batch_size, -1, input_dim])\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        outputs = gru_model(inputs)\n",
    "\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4981e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_gru, values_gru = evaluate(gru_model, val_set_gru, input_dim=input_dim)\n",
    "df_result_gru = format_predictions(predictions_gru, values_gru)\n",
    "result_metrics_gru = calculate_metrics(df_result_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b23d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Metrics of predicted values:\")\n",
    "for key, val in result_metrics_gru.items():\n",
    "    print(f\"{key.upper()}: {val:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0045c6ea",
   "metadata": {},
   "source": [
    "#### Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d96dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "import plotly.offline as pyo\n",
    "\n",
    "def plot_predictions(df_result):\n",
    "    data = []\n",
    "    \n",
    "    value = go.Scatter(\n",
    "        x=df_result.index,\n",
    "        y=df_result.value,\n",
    "        mode=\"lines\",\n",
    "        name=\"values\",\n",
    "        marker=dict(),\n",
    "        text=df_result.index,\n",
    "        line=dict(color=\"rgba(0,0,0, 0.3)\"),\n",
    "    )\n",
    "    \n",
    "    data.append(value)\n",
    "    \n",
    "    prediction = go.Scatter(\n",
    "        x=df_result.index,\n",
    "        y=df_result.prediction,\n",
    "        mode=\"lines\",\n",
    "        line={\"dash\": \"dot\"},\n",
    "        name=\"predictions\",\n",
    "        marker=dict(),\n",
    "        text=df_result.index,\n",
    "        opacity=0.8,\n",
    "    )\n",
    "    \n",
    "    data.append(prediction)\n",
    "    \n",
    "    layout = dict(\n",
    "        title=\"Predictions vs Actual Values for the dataset\",\n",
    "    )\n",
    "    \n",
    "    fig = dict(data=data, layout=layout)\n",
    "    iplot(fig)\n",
    "\n",
    "pyo.init_notebook_mode()\n",
    "plot_predictions(df_result_gru)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
